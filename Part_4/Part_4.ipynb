{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.multi_pred import DrugRes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator, Descriptors, AllChem, PandasTools\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Leaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow code performs feature selection based on the variance of columns in the dataset. The first step involves selecting only numeric (float) columns from the dataset using the select_dtypes method. This ensures that the data processed is compatible with the subsequent feature selection techniques, as non-numeric data cannot be directly used. This step is particularly important in bioinformatics or drug discovery, where numerical descriptors such as molecular weights or logP values are common. While this approach ensures compatibility, it may also exclude useful non-numeric columns, such as categorical data, which might require encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_GDSC = pd.read_csv('filtered_GDSC_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>HeavyAtomMolWt</th>\n",
       "      <th>ExactMolWt</th>\n",
       "      <th>MaxPartialCharge</th>\n",
       "      <th>...</th>\n",
       "      <th>VSA_EState4</th>\n",
       "      <th>VSA_EState5</th>\n",
       "      <th>VSA_EState6</th>\n",
       "      <th>VSA_EState7</th>\n",
       "      <th>VSA_EState8</th>\n",
       "      <th>VSA_EState9</th>\n",
       "      <th>FractionCSP3</th>\n",
       "      <th>MolLogP</th>\n",
       "      <th>MolMR</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>0.417884</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>0.162995</td>\n",
       "      <td>...</td>\n",
       "      <td>2.343161</td>\n",
       "      <td>4.443107</td>\n",
       "      <td>11.258046</td>\n",
       "      <td>6.994191</td>\n",
       "      <td>1.725695</td>\n",
       "      <td>3.250034</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>3.4051</td>\n",
       "      <td>111.9397</td>\n",
       "      <td>3.968757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>0.417884</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>0.162995</td>\n",
       "      <td>...</td>\n",
       "      <td>2.343161</td>\n",
       "      <td>4.443107</td>\n",
       "      <td>11.258046</td>\n",
       "      <td>6.994191</td>\n",
       "      <td>1.725695</td>\n",
       "      <td>3.250034</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>3.4051</td>\n",
       "      <td>111.9397</td>\n",
       "      <td>2.692768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>0.417884</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>0.162995</td>\n",
       "      <td>...</td>\n",
       "      <td>2.343161</td>\n",
       "      <td>4.443107</td>\n",
       "      <td>11.258046</td>\n",
       "      <td>6.994191</td>\n",
       "      <td>1.725695</td>\n",
       "      <td>3.250034</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>3.4051</td>\n",
       "      <td>111.9397</td>\n",
       "      <td>2.478678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>0.417884</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>0.162995</td>\n",
       "      <td>...</td>\n",
       "      <td>2.343161</td>\n",
       "      <td>4.443107</td>\n",
       "      <td>11.258046</td>\n",
       "      <td>6.994191</td>\n",
       "      <td>1.725695</td>\n",
       "      <td>3.250034</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>3.4051</td>\n",
       "      <td>111.9397</td>\n",
       "      <td>2.034050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>0.417884</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>0.162995</td>\n",
       "      <td>...</td>\n",
       "      <td>2.343161</td>\n",
       "      <td>4.443107</td>\n",
       "      <td>11.258046</td>\n",
       "      <td>6.994191</td>\n",
       "      <td>1.725695</td>\n",
       "      <td>3.250034</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>3.4051</td>\n",
       "      <td>111.9397</td>\n",
       "      <td>2.966952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170961</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>0.024463</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>0.692307</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>0.190493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340943</td>\n",
       "      <td>0.882212</td>\n",
       "      <td>13.464916</td>\n",
       "      <td>6.339041</td>\n",
       "      <td>1.822014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>2.4467</td>\n",
       "      <td>92.0593</td>\n",
       "      <td>5.353963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170962</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>0.024463</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>0.692307</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>0.190493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340943</td>\n",
       "      <td>0.882212</td>\n",
       "      <td>13.464916</td>\n",
       "      <td>6.339041</td>\n",
       "      <td>1.822014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>2.4467</td>\n",
       "      <td>92.0593</td>\n",
       "      <td>4.820567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170963</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>0.024463</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>0.692307</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>0.190493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340943</td>\n",
       "      <td>0.882212</td>\n",
       "      <td>13.464916</td>\n",
       "      <td>6.339041</td>\n",
       "      <td>1.822014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>2.4467</td>\n",
       "      <td>92.0593</td>\n",
       "      <td>5.785978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170964</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>0.024463</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>0.692307</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>0.190493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340943</td>\n",
       "      <td>0.882212</td>\n",
       "      <td>13.464916</td>\n",
       "      <td>6.339041</td>\n",
       "      <td>1.822014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>2.4467</td>\n",
       "      <td>92.0593</td>\n",
       "      <td>5.393454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170965</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>0.024463</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>0.692307</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>0.190493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340943</td>\n",
       "      <td>0.882212</td>\n",
       "      <td>13.464916</td>\n",
       "      <td>6.339041</td>\n",
       "      <td>1.822014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>2.4467</td>\n",
       "      <td>92.0593</td>\n",
       "      <td>5.099131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170966 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MaxAbsEStateIndex  MaxEStateIndex  MinAbsEStateIndex  MinEStateIndex  \\\n",
       "0                5.857397        5.857397           0.392174        0.392174   \n",
       "1                5.857397        5.857397           0.392174        0.392174   \n",
       "2                5.857397        5.857397           0.392174        0.392174   \n",
       "3                5.857397        5.857397           0.392174        0.392174   \n",
       "4                5.857397        5.857397           0.392174        0.392174   \n",
       "...                   ...             ...                ...             ...   \n",
       "170961          12.220360       12.220360           0.024463       -0.169023   \n",
       "170962          12.220360       12.220360           0.024463       -0.169023   \n",
       "170963          12.220360       12.220360           0.024463       -0.169023   \n",
       "170964          12.220360       12.220360           0.024463       -0.169023   \n",
       "170965          12.220360       12.220360           0.024463       -0.169023   \n",
       "\n",
       "             qed        SPS    MolWt  HeavyAtomMolWt  ExactMolWt  \\\n",
       "0       0.417884  10.517241  393.443         370.259  393.168856   \n",
       "1       0.417884  10.517241  393.443         370.259  393.168856   \n",
       "2       0.417884  10.517241  393.443         370.259  393.168856   \n",
       "3       0.417884  10.517241  393.443         370.259  393.168856   \n",
       "4       0.417884  10.517241  393.443         370.259  393.168856   \n",
       "...          ...        ...      ...             ...         ...   \n",
       "170961  0.692307  22.500000  321.380         302.228  321.147727   \n",
       "170962  0.692307  22.500000  321.380         302.228  321.147727   \n",
       "170963  0.692307  22.500000  321.380         302.228  321.147727   \n",
       "170964  0.692307  22.500000  321.380         302.228  321.147727   \n",
       "170965  0.692307  22.500000  321.380         302.228  321.147727   \n",
       "\n",
       "        MaxPartialCharge  ...  VSA_EState4  VSA_EState5  VSA_EState6  \\\n",
       "0               0.162995  ...     2.343161     4.443107    11.258046   \n",
       "1               0.162995  ...     2.343161     4.443107    11.258046   \n",
       "2               0.162995  ...     2.343161     4.443107    11.258046   \n",
       "3               0.162995  ...     2.343161     4.443107    11.258046   \n",
       "4               0.162995  ...     2.343161     4.443107    11.258046   \n",
       "...                  ...  ...          ...          ...          ...   \n",
       "170961          0.190493  ...     0.340943     0.882212    13.464916   \n",
       "170962          0.190493  ...     0.340943     0.882212    13.464916   \n",
       "170963          0.190493  ...     0.340943     0.882212    13.464916   \n",
       "170964          0.190493  ...     0.340943     0.882212    13.464916   \n",
       "170965          0.190493  ...     0.340943     0.882212    13.464916   \n",
       "\n",
       "        VSA_EState7  VSA_EState8  VSA_EState9  FractionCSP3  MolLogP  \\\n",
       "0          6.994191     1.725695     3.250034      0.272727   3.4051   \n",
       "1          6.994191     1.725695     3.250034      0.272727   3.4051   \n",
       "2          6.994191     1.725695     3.250034      0.272727   3.4051   \n",
       "3          6.994191     1.725695     3.250034      0.272727   3.4051   \n",
       "4          6.994191     1.725695     3.250034      0.272727   3.4051   \n",
       "...             ...          ...          ...           ...      ...   \n",
       "170961     6.339041     1.822014     0.000000      0.263158   2.4467   \n",
       "170962     6.339041     1.822014     0.000000      0.263158   2.4467   \n",
       "170963     6.339041     1.822014     0.000000      0.263158   2.4467   \n",
       "170964     6.339041     1.822014     0.000000      0.263158   2.4467   \n",
       "170965     6.339041     1.822014     0.000000      0.263158   2.4467   \n",
       "\n",
       "           MolMR         Y  \n",
       "0       111.9397  3.968757  \n",
       "1       111.9397  2.692768  \n",
       "2       111.9397  2.478678  \n",
       "3       111.9397  2.034050  \n",
       "4       111.9397  2.966952  \n",
       "...          ...       ...  \n",
       "170961   92.0593  5.353963  \n",
       "170962   92.0593  4.820567  \n",
       "170963   92.0593  5.785978  \n",
       "170964   92.0593  5.393454  \n",
       "170965   92.0593  5.099131  \n",
       "\n",
       "[170966 rows x 99 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_final=filtered_GDSC.select_dtypes(include=float)\n",
    "dataset_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a variance threshold is applied to the dataset. This process uses a VarianceThreshold object with a set threshold of 0.05 to eliminate features with very low variance. Features with minimal variance are unlikely to provide meaningful distinctions between data points and thus add little value to machine learning models. By filtering out these features, the dimensionality of the dataset is reduced, which in turn speeds up model training and enhances interpretability. However, this approach has a downside—features with low variance but biological significance (such as rare biomarkers) might be inadvertently discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = VarianceThreshold(threshold=0.05)\n",
    "selected_dataset = selector.fit_transform(dataset_final)\n",
    "selected_columns = dataset_final.columns[selector.get_support()]\n",
    "dataset_filtrado = pd.DataFrame(dataset_final, columns=selected_columns)\n",
    "#print(\"Shape do dataset após aplicar o Variance Threshold:\", selected_dataset.shape)\n",
    "#print(\"Colunas selecionadas:\", selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtrado.to_csv('dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>HeavyAtomMolWt</th>\n",
       "      <th>ExactMolWt</th>\n",
       "      <th>FpDensityMorgan2</th>\n",
       "      <th>FpDensityMorgan3</th>\n",
       "      <th>AvgIpc</th>\n",
       "      <th>...</th>\n",
       "      <th>VSA_EState3</th>\n",
       "      <th>VSA_EState4</th>\n",
       "      <th>VSA_EState5</th>\n",
       "      <th>VSA_EState6</th>\n",
       "      <th>VSA_EState7</th>\n",
       "      <th>VSA_EState8</th>\n",
       "      <th>VSA_EState9</th>\n",
       "      <th>MolLogP</th>\n",
       "      <th>MolMR</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>1.655172</td>\n",
       "      <td>2.344828</td>\n",
       "      <td>2.745576</td>\n",
       "      <td>...</td>\n",
       "      <td>4.096557</td>\n",
       "      <td>2.343161</td>\n",
       "      <td>4.443107</td>\n",
       "      <td>11.258046</td>\n",
       "      <td>6.994191</td>\n",
       "      <td>1.725695</td>\n",
       "      <td>3.250034</td>\n",
       "      <td>3.4051</td>\n",
       "      <td>111.9397</td>\n",
       "      <td>3.968757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>1.655172</td>\n",
       "      <td>2.344828</td>\n",
       "      <td>2.745576</td>\n",
       "      <td>...</td>\n",
       "      <td>4.096557</td>\n",
       "      <td>2.343161</td>\n",
       "      <td>4.443107</td>\n",
       "      <td>11.258046</td>\n",
       "      <td>6.994191</td>\n",
       "      <td>1.725695</td>\n",
       "      <td>3.250034</td>\n",
       "      <td>3.4051</td>\n",
       "      <td>111.9397</td>\n",
       "      <td>2.692768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>1.655172</td>\n",
       "      <td>2.344828</td>\n",
       "      <td>2.745576</td>\n",
       "      <td>...</td>\n",
       "      <td>4.096557</td>\n",
       "      <td>2.343161</td>\n",
       "      <td>4.443107</td>\n",
       "      <td>11.258046</td>\n",
       "      <td>6.994191</td>\n",
       "      <td>1.725695</td>\n",
       "      <td>3.250034</td>\n",
       "      <td>3.4051</td>\n",
       "      <td>111.9397</td>\n",
       "      <td>2.478678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>1.655172</td>\n",
       "      <td>2.344828</td>\n",
       "      <td>2.745576</td>\n",
       "      <td>...</td>\n",
       "      <td>4.096557</td>\n",
       "      <td>2.343161</td>\n",
       "      <td>4.443107</td>\n",
       "      <td>11.258046</td>\n",
       "      <td>6.994191</td>\n",
       "      <td>1.725695</td>\n",
       "      <td>3.250034</td>\n",
       "      <td>3.4051</td>\n",
       "      <td>111.9397</td>\n",
       "      <td>2.034050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>1.655172</td>\n",
       "      <td>2.344828</td>\n",
       "      <td>2.745576</td>\n",
       "      <td>...</td>\n",
       "      <td>4.096557</td>\n",
       "      <td>2.343161</td>\n",
       "      <td>4.443107</td>\n",
       "      <td>11.258046</td>\n",
       "      <td>6.994191</td>\n",
       "      <td>1.725695</td>\n",
       "      <td>3.250034</td>\n",
       "      <td>3.4051</td>\n",
       "      <td>111.9397</td>\n",
       "      <td>2.966952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170961</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>1.958333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.210046</td>\n",
       "      <td>...</td>\n",
       "      <td>9.756959</td>\n",
       "      <td>0.340943</td>\n",
       "      <td>0.882212</td>\n",
       "      <td>13.464916</td>\n",
       "      <td>6.339041</td>\n",
       "      <td>1.822014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.4467</td>\n",
       "      <td>92.0593</td>\n",
       "      <td>5.353963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170962</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>1.958333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.210046</td>\n",
       "      <td>...</td>\n",
       "      <td>9.756959</td>\n",
       "      <td>0.340943</td>\n",
       "      <td>0.882212</td>\n",
       "      <td>13.464916</td>\n",
       "      <td>6.339041</td>\n",
       "      <td>1.822014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.4467</td>\n",
       "      <td>92.0593</td>\n",
       "      <td>4.820567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170963</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>1.958333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.210046</td>\n",
       "      <td>...</td>\n",
       "      <td>9.756959</td>\n",
       "      <td>0.340943</td>\n",
       "      <td>0.882212</td>\n",
       "      <td>13.464916</td>\n",
       "      <td>6.339041</td>\n",
       "      <td>1.822014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.4467</td>\n",
       "      <td>92.0593</td>\n",
       "      <td>5.785978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170964</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>1.958333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.210046</td>\n",
       "      <td>...</td>\n",
       "      <td>9.756959</td>\n",
       "      <td>0.340943</td>\n",
       "      <td>0.882212</td>\n",
       "      <td>13.464916</td>\n",
       "      <td>6.339041</td>\n",
       "      <td>1.822014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.4467</td>\n",
       "      <td>92.0593</td>\n",
       "      <td>5.393454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170965</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>1.958333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.210046</td>\n",
       "      <td>...</td>\n",
       "      <td>9.756959</td>\n",
       "      <td>0.340943</td>\n",
       "      <td>0.882212</td>\n",
       "      <td>13.464916</td>\n",
       "      <td>6.339041</td>\n",
       "      <td>1.822014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.4467</td>\n",
       "      <td>92.0593</td>\n",
       "      <td>5.099131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170966 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MaxAbsEStateIndex  MaxEStateIndex  MinEStateIndex        SPS    MolWt  \\\n",
       "0                5.857397        5.857397        0.392174  10.517241  393.443   \n",
       "1                5.857397        5.857397        0.392174  10.517241  393.443   \n",
       "2                5.857397        5.857397        0.392174  10.517241  393.443   \n",
       "3                5.857397        5.857397        0.392174  10.517241  393.443   \n",
       "4                5.857397        5.857397        0.392174  10.517241  393.443   \n",
       "...                   ...             ...             ...        ...      ...   \n",
       "170961          12.220360       12.220360       -0.169023  22.500000  321.380   \n",
       "170962          12.220360       12.220360       -0.169023  22.500000  321.380   \n",
       "170963          12.220360       12.220360       -0.169023  22.500000  321.380   \n",
       "170964          12.220360       12.220360       -0.169023  22.500000  321.380   \n",
       "170965          12.220360       12.220360       -0.169023  22.500000  321.380   \n",
       "\n",
       "        HeavyAtomMolWt  ExactMolWt  FpDensityMorgan2  FpDensityMorgan3  \\\n",
       "0              370.259  393.168856          1.655172          2.344828   \n",
       "1              370.259  393.168856          1.655172          2.344828   \n",
       "2              370.259  393.168856          1.655172          2.344828   \n",
       "3              370.259  393.168856          1.655172          2.344828   \n",
       "4              370.259  393.168856          1.655172          2.344828   \n",
       "...                ...         ...               ...               ...   \n",
       "170961         302.228  321.147727          1.958333          2.666667   \n",
       "170962         302.228  321.147727          1.958333          2.666667   \n",
       "170963         302.228  321.147727          1.958333          2.666667   \n",
       "170964         302.228  321.147727          1.958333          2.666667   \n",
       "170965         302.228  321.147727          1.958333          2.666667   \n",
       "\n",
       "          AvgIpc  ...  VSA_EState3  VSA_EState4  VSA_EState5  VSA_EState6  \\\n",
       "0       2.745576  ...     4.096557     2.343161     4.443107    11.258046   \n",
       "1       2.745576  ...     4.096557     2.343161     4.443107    11.258046   \n",
       "2       2.745576  ...     4.096557     2.343161     4.443107    11.258046   \n",
       "3       2.745576  ...     4.096557     2.343161     4.443107    11.258046   \n",
       "4       2.745576  ...     4.096557     2.343161     4.443107    11.258046   \n",
       "...          ...  ...          ...          ...          ...          ...   \n",
       "170961  3.210046  ...     9.756959     0.340943     0.882212    13.464916   \n",
       "170962  3.210046  ...     9.756959     0.340943     0.882212    13.464916   \n",
       "170963  3.210046  ...     9.756959     0.340943     0.882212    13.464916   \n",
       "170964  3.210046  ...     9.756959     0.340943     0.882212    13.464916   \n",
       "170965  3.210046  ...     9.756959     0.340943     0.882212    13.464916   \n",
       "\n",
       "        VSA_EState7  VSA_EState8  VSA_EState9  MolLogP     MolMR         Y  \n",
       "0          6.994191     1.725695     3.250034   3.4051  111.9397  3.968757  \n",
       "1          6.994191     1.725695     3.250034   3.4051  111.9397  2.692768  \n",
       "2          6.994191     1.725695     3.250034   3.4051  111.9397  2.478678  \n",
       "3          6.994191     1.725695     3.250034   3.4051  111.9397  2.034050  \n",
       "4          6.994191     1.725695     3.250034   3.4051  111.9397  2.966952  \n",
       "...             ...          ...          ...      ...       ...       ...  \n",
       "170961     6.339041     1.822014     0.000000   2.4467   92.0593  5.353963  \n",
       "170962     6.339041     1.822014     0.000000   2.4467   92.0593  4.820567  \n",
       "170963     6.339041     1.822014     0.000000   2.4467   92.0593  5.785978  \n",
       "170964     6.339041     1.822014     0.000000   2.4467   92.0593  5.393454  \n",
       "170965     6.339041     1.822014     0.000000   2.4467   92.0593  5.099131  \n",
       "\n",
       "[170966 rows x 89 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our dataset has gone from 99 columns to 88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enconding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to prepare the dataset for deep learning by encoding specific features, such as the Tissue and Morgan Fingerprints, to ensure compatibility with algorithms that require numerical input. Initially, the Tissue and Morgan Fingerprints columns are extracted from the dataset to isolate the data that requires transformation. The Tissue column represents the biological origin of the cell lines (e.g., lung, blood), which is crucial for understanding the biological context of drug responses. The Morgan Fingerprints, on the other hand, provide numerical representations of molecular structures, encoding essential structural information used to predict drug activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_data = filtered_GDSC['Tissue']\n",
    "morgan_fingerprints = filtered_GDSC['morgan_fingerprints']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Morgan Fingerprints are then expanded into a structured numerical format. Each fingerprint, typically stored as a list or array, is unpacked into individual columns (morgan_0, morgan_1, etc.) to ensure compatibility with deep learning models, which require fixed-size numerical input. While this approach captures detailed molecular information, it also increases the dimensionality of the dataset, which may result in higher computational demands. The expansion of fingerprints is biologically significant as it allows algorithms to analyze drugs based on their structural differences or similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "morgan_encodings = np.vstack(morgan_fingerprints)\n",
    "morgan_columns = [f'morgan_{i}' for i in range(morgan_encodings.shape[1])]\n",
    "morgan_df = pd.DataFrame(morgan_encodings, columns=morgan_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tissue column undergoes one-hot encoding, transforming the categorical data into binary format. Each unique tissue type is represented as a separate column, with each row containing a 1 for the corresponding tissue type and 0s elsewhere. This step ensures that categorical information is preserved and converted into a numerical format without introducing any ordinal assumptions. While this process increases the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "tissue_encodings = one_hot_encoder.fit_transform(tissue_data.values.reshape(-1, 1))\n",
    "tissue_columns = [f'tissue_{cat}' for cat in one_hot_encoder.categories_[0]]\n",
    "tissue_df = pd.DataFrame(tissue_encodings, columns=tissue_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing the Tissue and Morgan Fingerprints data, they are concatenated with the filtered dataset, creating an augmented dataset that integrates molecular and biological features. This comprehensive dataset is ready for model training and ensures that all relevant data is combined into one unified structure. However, the integration increases the overall size of the dataset, which can require more computational resources. The combined dataset allows the model to leverage both the chemical and biological contexts, enhancing its predictive capabilities and making it more robust for complex tasks such as predicting drug efficacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = pd.concat([dataset_filtrado, morgan_df, tissue_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'augmented_filtered_dataset.csv'\n",
    "augmented_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, its necessary splitting the dataset into training and testing sets and normalizing the features to ensure compatibility with the algorithms. First, the target variable (Y), which represents the outcome or label to predict, is separated from the feature set. The target variable is extracted as target_data, while the rest of the dataset is stored as augmented_data. This separation ensures that the deep learning model can be trained with input features and validated against the target labels without introducing leakage. In the context of bioinformatics, the target often represents biological responses, such as drug efficacy, making its isolation critical for meaningful predictions. The dataset is then split into training and testing sets using the train_test_split function. The training set, comprising 80% of the data, is used to train the model, while the remaining 20% is reserved for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas no dataset augmentado (X): 170966, Colunas: 2149\n",
      "Linhas no target_data (Y): 170966\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target_data = augmented_data['Y'].values  \n",
    "augmented_data = augmented_data.drop(columns=['Y'])  \n",
    "\n",
    "\n",
    "print(f\"Linhas no dataset augmentado (X): {augmented_data.shape[0]}, Colunas: {augmented_data.shape[1]}\")\n",
    "print(f\"Linhas no target_data (Y): {target_data.shape[0]}\")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_data.values, target_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the features are normalized using the StandardScaler, which standardizes the data by scaling it to have a zero mean and unit variance. The scaler is fitted on the training set and then applied to both the training and testing sets. This ensures that the testing data is scaled consistently with the training data, preventing data leakage. Normalization is essential because many machine learning algorithms are sensitive to the scale of input features, and standardizing them improves model convergence and performance. In biological datasets, where features may include diverse molecular and tissue-related data with varying ranges, normalization ensures that no single feature dominates due to its magnitude, allowing the model to focus on meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "its time to defines, compiles, and trains a Deep Neural Network (DNN) for predicting outcomes based on the input dataset. The first part defines the DNN architecture using a function called create_dnn. The model is built sequentially and consists of three layers: the first layer is a dense (fully connected) layer with 128 units and ReLU activation, followed by a dropout layer that randomly drops 30% of the connections during training to reduce overfitting. The second layer is another dense layer with 64 units and ReLU activation, also followed by a dropout layer. The final layer is a single-unit dense layer with linear activation, which is suitable for regression tasks where the goal is to predict continuous values. The model is compiled using the Adam optimizer, which adjusts learning rates dynamically to improve convergence, mean squared error (MSE) as the loss function to minimize, and mean absolute error (MAE) as a performance metric for evaluation. This architecture is particularly designed for regression tasks that involve complex, non-linear relationships, making it well-suited for biological applications, such as predicting drug responses or molecular activities. The dropout layers add robustness by preventing the model from overfitting the training data, while the use of ReLU activation ensures computational efficiency and avoids the vanishing gradient problem common in deeper networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "def create_dnn(input_shape):\n",
    "    model = Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    print(\"DNN Summary:\")\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_dnn function is then used to instantiate the model. The model is trained using the fit method, with 20% of the training data reserved for validation to monitor the model's generalization performance. The training is conducted over 50 epochs (iterations over the dataset) with a batch size of 32, meaning the model updates its weights after every 32 samples. The verbose output allows real-time monitoring of the training and validation performance metrics. This training process adjusts the model's parameters (weights and biases) to minimize the error on the training data while ensuring it generalizes well to unseen data through validation monitoring. The combination of layers, dropout, and the Adam optimizer creates a robust and efficient model capable of learning complex patterns in the dataset. However, the model's performance heavily depends on the choice of hyperparameters, such as the number of layers, dropout rates, and learning rates, which need to be carefully tuned. In the context of biological datasets, this DNN architecture can effectively predict outcomes such as drug sensitivity or therapeutic responses by leveraging molecular and tissue-specific features, capturing the underlying biological variability in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">275,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m275,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">283,521</span> (1.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m283,521\u001b[0m (1.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">283,521</span> (1.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m283,521\u001b[0m (1.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 3.3892 - mae: 1.3945 - val_loss: 2.0537 - val_mae: 1.0834\n",
      "Epoch 2/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.2943 - mae: 1.1520 - val_loss: 2.0259 - val_mae: 1.0702\n",
      "Epoch 3/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.2721 - mae: 1.1484 - val_loss: 2.0362 - val_mae: 1.0828\n",
      "Epoch 4/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.1860 - mae: 1.1227 - val_loss: 1.9273 - val_mae: 1.0480\n",
      "Epoch 5/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.1968 - mae: 1.1238 - val_loss: 2.0799 - val_mae: 1.0834\n",
      "Epoch 6/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.1706 - mae: 1.1144 - val_loss: 1.9476 - val_mae: 1.0633\n",
      "Epoch 7/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.1373 - mae: 1.1056 - val_loss: 1.8937 - val_mae: 1.0367\n",
      "Epoch 8/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.1364 - mae: 1.1049 - val_loss: 2.3989 - val_mae: 1.1549\n",
      "Epoch 9/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.1150 - mae: 1.1011 - val_loss: 1.9807 - val_mae: 1.0612\n",
      "Epoch 10/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.1129 - mae: 1.0989 - val_loss: 2.3004 - val_mae: 1.1793\n",
      "Epoch 11/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.1090 - mae: 1.0962 - val_loss: 1.9664 - val_mae: 1.0585\n",
      "Epoch 12/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0915 - mae: 1.0941 - val_loss: 1.9512 - val_mae: 1.0572\n",
      "Epoch 13/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0946 - mae: 1.0923 - val_loss: 1.9517 - val_mae: 1.0647\n",
      "Epoch 14/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0943 - mae: 1.0935 - val_loss: 1.9580 - val_mae: 1.0563\n",
      "Epoch 15/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0776 - mae: 1.0906 - val_loss: 2.0144 - val_mae: 1.0778\n",
      "Epoch 16/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0800 - mae: 1.0903 - val_loss: 2.1546 - val_mae: 1.1015\n",
      "Epoch 17/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0730 - mae: 1.0881 - val_loss: 2.1348 - val_mae: 1.1133\n",
      "Epoch 18/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0673 - mae: 1.0859 - val_loss: 2.0089 - val_mae: 1.0621\n",
      "Epoch 19/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0936 - mae: 1.0907 - val_loss: 1.9672 - val_mae: 1.0630\n",
      "Epoch 20/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0730 - mae: 1.0902 - val_loss: 1.9506 - val_mae: 1.0675\n",
      "Epoch 21/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0793 - mae: 1.0889 - val_loss: 2.1182 - val_mae: 1.0942\n",
      "Epoch 22/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0515 - mae: 1.0816 - val_loss: 2.0844 - val_mae: 1.0873\n",
      "Epoch 23/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0634 - mae: 1.0875 - val_loss: 1.9198 - val_mae: 1.0523\n",
      "Epoch 24/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0817 - mae: 1.0884 - val_loss: 1.9727 - val_mae: 1.0658\n",
      "Epoch 25/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0664 - mae: 1.0850 - val_loss: 1.8588 - val_mae: 1.0293\n",
      "Epoch 26/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0522 - mae: 1.0801 - val_loss: 2.0298 - val_mae: 1.0792\n",
      "Epoch 27/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0987 - mae: 1.0933 - val_loss: 2.2238 - val_mae: 1.1395\n",
      "Epoch 28/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0283 - mae: 1.0766 - val_loss: 1.9902 - val_mae: 1.0619\n",
      "Epoch 29/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0361 - mae: 1.0770 - val_loss: 2.1437 - val_mae: 1.1179\n",
      "Epoch 30/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0420 - mae: 1.0810 - val_loss: 2.0597 - val_mae: 1.0873\n",
      "Epoch 31/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0414 - mae: 1.0801 - val_loss: 2.1925 - val_mae: 1.1040\n",
      "Epoch 32/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0362 - mae: 1.0753 - val_loss: 2.1259 - val_mae: 1.0948\n",
      "Epoch 33/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0379 - mae: 1.0772 - val_loss: 1.8859 - val_mae: 1.0388\n",
      "Epoch 34/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0187 - mae: 1.0729 - val_loss: 1.9154 - val_mae: 1.0481\n",
      "Epoch 35/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0312 - mae: 1.0764 - val_loss: 1.9763 - val_mae: 1.0681\n",
      "Epoch 36/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0121 - mae: 1.0739 - val_loss: 1.9870 - val_mae: 1.0696\n",
      "Epoch 37/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 1.9965 - mae: 1.0692 - val_loss: 2.0355 - val_mae: 1.0921\n",
      "Epoch 38/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0207 - mae: 1.0757 - val_loss: 2.0791 - val_mae: 1.0861\n",
      "Epoch 39/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0611 - mae: 1.0848 - val_loss: 1.8767 - val_mae: 1.0363\n",
      "Epoch 40/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0172 - mae: 1.0730 - val_loss: 1.8798 - val_mae: 1.0398\n",
      "Epoch 41/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0427 - mae: 1.0793 - val_loss: 1.9361 - val_mae: 1.0484\n",
      "Epoch 42/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0318 - mae: 1.0778 - val_loss: 1.9424 - val_mae: 1.0587\n",
      "Epoch 43/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0268 - mae: 1.0750 - val_loss: 1.9908 - val_mae: 1.0738\n",
      "Epoch 44/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0397 - mae: 1.0772 - val_loss: 1.9168 - val_mae: 1.0424\n",
      "Epoch 45/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0224 - mae: 1.0723 - val_loss: 1.9935 - val_mae: 1.0812\n",
      "Epoch 46/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0151 - mae: 1.0719 - val_loss: 1.9361 - val_mae: 1.0563\n",
      "Epoch 47/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0256 - mae: 1.0768 - val_loss: 1.8909 - val_mae: 1.0403\n",
      "Epoch 48/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0075 - mae: 1.0701 - val_loss: 2.1128 - val_mae: 1.0931\n",
      "Epoch 49/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0288 - mae: 1.0779 - val_loss: 1.8519 - val_mae: 1.0271\n",
      "Epoch 50/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.0110 - mae: 1.0721 - val_loss: 2.1845 - val_mae: 1.1154\n"
     ]
    }
   ],
   "source": [
    "input_shape_dnn = (X_train.shape[1],)\n",
    "\n",
    "dnn_model = create_dnn(input_shape_dnn)\n",
    "\n",
    "dnn_history = dnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code evaluates the performance of the trained Deep Neural Network (DNN) model on the test dataset and contextualizes the results by analyzing the target variable's distribution. First, the model's performance is assessed using the evaluate function, which calculates the Mean Absolute Error (MAE) on the test set. The MAE quantifies the average absolute difference between the predicted and actual values, providing a straightforward measure of the model's accuracy. In this case, the reported MAE on the test set is 1.1123, indicating the average prediction error. This step is essential to assess how well the model generalizes to unseen data and provides a practical sense of prediction accuracy in the same units as the target variable. The MAE is particularly useful for regression tasks as it is not overly sensitive to outliers, unlike other metrics like Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Mean Absolute Error (MAE) no conjunto de teste: 1.1123\n"
     ]
    }
   ],
   "source": [
    "dnn_eval = dnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"DNN Mean Absolute Error (MAE) no conjunto de teste: {dnn_eval[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de Y: 2.0095\n",
      "Desvio padrão de Y: 2.7165\n",
      "Valor mínimo de Y: -9.9334\n",
      "Valor máximo de Y: 12.3591\n"
     ]
    }
   ],
   "source": [
    "print(f\"Média de Y: {np.mean(target_data):.4f}\")\n",
    "print(f\"Desvio padrão de Y: {np.std(target_data):.4f}\")\n",
    "print(f\"Valor mínimo de Y: {np.min(target_data):.4f}\")\n",
    "print(f\"Valor máximo de Y: {np.max(target_data):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the evaluation of the DNN model demonstrates its strong predictive performance, with a low MAE and relative MAE, indicating that the model's errors are small compared to the variability of the target variable. The descriptive statistics provide additional context for interpreting these results, ensuring that the predictions are meaningful in the biological context. This evaluation confirms the utility of the model for applications such as predicting drug responses or other biological outcomes, where accuracy and reliability are critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE relativo ao intervalo de Y: 4.99%\n"
     ]
    }
   ],
   "source": [
    "mae_relative = dnn_eval[1] / (np.max(target_data) - np.min(target_data))\n",
    "print(f\"MAE relativo ao intervalo de Y: {mae_relative:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram reveals the shape of the data distribution. In this case, the target variable appears to follow a roughly symmetric distribution centered near zero, with a gradual tapering of frequency toward the extremes. This suggests that the data has a bell-shaped pattern, which could indicate a normal-like distribution, though it would require further statistical tests to confirm. Understanding the distribution of the target variable is critical because it impacts the choice of machine learning models and evaluation metrics. For instance, if the distribution were highly skewed or multimodal, different modeling strategies might be required.\n",
    "\n",
    "From a biological perspective, this distribution provides insights into the variability of the outcome being studied, such as drug response or a biological measurement. The high frequency of values near the center suggests that most outcomes cluster around a typical value, while the lower frequencies at the extremes indicate rarer, potentially significant biological events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(target_data, bins=50, edgecolor='k')\n",
    "plt.title(\"Distribuição da Variável Alvo (Y)\")\n",
    "plt.xlabel(\"Valores de Y\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_cnn function defines the CNN architecture. It begins by reshaping the input data to a format suitable for convolutional operations, treating the features as a 1D sequence. The model includes two 1D convolutional layers with 32 and 64 filters, respectively, each using a kernel size of 3 and ReLU activation. These layers extract local patterns from the input data, such as dependencies or correlations between features. Each convolutional layer is followed by a MaxPooling layer, which downsamples the data to reduce its dimensionality while retaining the most significant features, thereby improving computational efficiency.\n",
    "\n",
    "After the convolutional layers, the data is flattened into a single vector to prepare it for the fully connected layers. A dense layer with 64 neurons and ReLU activation is included to learn high-level feature representations. A dropout layer with a 30% dropout rate follows, helping to reduce overfitting by randomly deactivating neurons during training. The final layer is a dense output layer with one neuron and a linear activation function, which is appropriate for regression tasks where the output is a continuous variable. The model is compiled using the Adam optimizer, which dynamically adjusts learning rates for faster convergence, the mean squared error (MSE) as the loss function, and the mean absolute error (MAE) as a performance metric for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(input_shape):\n",
    "    model = Sequential([\n",
    "        layers.Reshape((input_shape[0], 1), input_shape=input_shape),\n",
    "        layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    print(\"CNN Summary:\")\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_cnn function is called to instantiate the model, and the model is trained using the fit method. During training, 20% of the data is reserved for validation, allowing the model's generalization performance to be monitored. The training runs for 50 epochs, with a batch size of 32, enabling frequent weight updates and faster convergence. The training process adjusts the model's parameters, including filters, weights, and biases, to minimize the error on the training data while maintaining good performance on the validation set.\n",
    "\n",
    "This CNN architecture is particularly advantageous because it leverages convolutional layers to extract hierarchical patterns, making it ideal for datasets with complex dependencies. The pooling layers and dropout further enhance its generalization by reducing overfitting and computational complexity. However, training CNNs can be computationally intensive, and the choice of hyperparameters, such as the number of filters, kernel size, and dropout rate, significantly impacts performance. From a biological perspective, this CNN model is well-suited for analyzing molecular descriptors or gene expression data, as it can capture local dependencies and intricate relationships between features. This makes it highly effective for tasks like predicting drug efficacy or modeling biological responses, where understanding subtle patterns in the data is crucial for generating meaningful predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2149</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2147</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1073</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1071</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">535</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34240</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,191,424</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2149\u001b[0m, \u001b[38;5;34m1\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2147\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1073\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1071\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │         \u001b[38;5;34m6,208\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m535\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34240\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m2,191,424\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,197,825</span> (8.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,197,825\u001b[0m (8.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,197,825</span> (8.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,197,825\u001b[0m (8.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 25ms/step - loss: 3.1092 - mae: 1.3336 - val_loss: 2.0390 - val_mae: 1.0882\n",
      "Epoch 2/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 26ms/step - loss: 2.3825 - mae: 1.1748 - val_loss: 1.9383 - val_mae: 1.0478\n",
      "Epoch 3/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 26ms/step - loss: 2.2509 - mae: 1.1420 - val_loss: 1.9782 - val_mae: 1.0691\n",
      "Epoch 4/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 26ms/step - loss: 2.2310 - mae: 1.1340 - val_loss: 1.9144 - val_mae: 1.0421\n",
      "Epoch 5/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 26ms/step - loss: 2.2030 - mae: 1.1290 - val_loss: 1.8880 - val_mae: 1.0351\n",
      "Epoch 6/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 27ms/step - loss: 2.1642 - mae: 1.1169 - val_loss: 1.9263 - val_mae: 1.0453\n",
      "Epoch 7/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 27ms/step - loss: 2.1666 - mae: 1.1155 - val_loss: 1.8969 - val_mae: 1.0408\n",
      "Epoch 8/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 27ms/step - loss: 2.1455 - mae: 1.1128 - val_loss: 1.8829 - val_mae: 1.0366\n",
      "Epoch 9/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 26ms/step - loss: 2.1314 - mae: 1.1074 - val_loss: 1.8909 - val_mae: 1.0359\n",
      "Epoch 10/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 27ms/step - loss: 2.1277 - mae: 1.1044 - val_loss: 1.9010 - val_mae: 1.0532\n",
      "Epoch 11/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 28ms/step - loss: 2.1293 - mae: 1.1058 - val_loss: 1.8848 - val_mae: 1.0410\n",
      "Epoch 12/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 27ms/step - loss: 2.1256 - mae: 1.1049 - val_loss: 1.8726 - val_mae: 1.0269\n",
      "Epoch 13/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 27ms/step - loss: 2.1045 - mae: 1.0995 - val_loss: 1.8569 - val_mae: 1.0270\n",
      "Epoch 14/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 27ms/step - loss: 2.1073 - mae: 1.0974 - val_loss: 1.8425 - val_mae: 1.0209\n",
      "Epoch 15/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 27ms/step - loss: 2.1110 - mae: 1.1020 - val_loss: 1.8600 - val_mae: 1.0244\n",
      "Epoch 16/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 27ms/step - loss: 2.0888 - mae: 1.0919 - val_loss: 1.8571 - val_mae: 1.0321\n",
      "Epoch 17/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 27ms/step - loss: 2.0875 - mae: 1.0940 - val_loss: 1.8519 - val_mae: 1.0319\n",
      "Epoch 18/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 27ms/step - loss: 2.1026 - mae: 1.0984 - val_loss: 1.8433 - val_mae: 1.0168\n",
      "Epoch 19/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 27ms/step - loss: 2.0789 - mae: 1.0921 - val_loss: 1.8296 - val_mae: 1.0162\n",
      "Epoch 20/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 27ms/step - loss: 2.0840 - mae: 1.0933 - val_loss: 1.8811 - val_mae: 1.0383\n",
      "Epoch 21/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 28ms/step - loss: 2.0954 - mae: 1.0941 - val_loss: 1.8617 - val_mae: 1.0310\n",
      "Epoch 22/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 28ms/step - loss: 2.0778 - mae: 1.0914 - val_loss: 1.8403 - val_mae: 1.0230\n",
      "Epoch 23/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 28ms/step - loss: 2.0923 - mae: 1.0937 - val_loss: 1.8281 - val_mae: 1.0146\n",
      "Epoch 24/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 27ms/step - loss: 2.0885 - mae: 1.0927 - val_loss: 1.8766 - val_mae: 1.0421\n",
      "Epoch 25/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 27ms/step - loss: 2.0635 - mae: 1.0879 - val_loss: 1.8236 - val_mae: 1.0178\n",
      "Epoch 26/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 28ms/step - loss: 2.0810 - mae: 1.0922 - val_loss: 1.8789 - val_mae: 1.0414\n",
      "Epoch 27/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 28ms/step - loss: 2.0580 - mae: 1.0844 - val_loss: 1.8286 - val_mae: 1.0176\n",
      "Epoch 28/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 28ms/step - loss: 2.0685 - mae: 1.0888 - val_loss: 1.8437 - val_mae: 1.0278\n",
      "Epoch 29/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 28ms/step - loss: 2.0738 - mae: 1.0891 - val_loss: 1.8685 - val_mae: 1.0356\n",
      "Epoch 30/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 28ms/step - loss: 2.0758 - mae: 1.0887 - val_loss: 1.8505 - val_mae: 1.0298\n",
      "Epoch 31/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 28ms/step - loss: 2.0607 - mae: 1.0878 - val_loss: 1.8778 - val_mae: 1.0427\n",
      "Epoch 32/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 28ms/step - loss: 2.0651 - mae: 1.0908 - val_loss: 1.8511 - val_mae: 1.0246\n",
      "Epoch 33/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 28ms/step - loss: 2.0745 - mae: 1.0901 - val_loss: 1.8288 - val_mae: 1.0155\n",
      "Epoch 34/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 28ms/step - loss: 2.0625 - mae: 1.0866 - val_loss: 1.8809 - val_mae: 1.0399\n",
      "Epoch 35/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 28ms/step - loss: 2.0746 - mae: 1.0908 - val_loss: 1.8217 - val_mae: 1.0185\n",
      "Epoch 36/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 28ms/step - loss: 2.0971 - mae: 1.0946 - val_loss: 1.8196 - val_mae: 1.0168\n",
      "Epoch 37/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 28ms/step - loss: 2.0940 - mae: 1.0966 - val_loss: 1.8730 - val_mae: 1.0398\n",
      "Epoch 38/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 28ms/step - loss: 2.0720 - mae: 1.0891 - val_loss: 1.8293 - val_mae: 1.0140\n",
      "Epoch 39/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 28ms/step - loss: 2.0519 - mae: 1.0840 - val_loss: 1.8160 - val_mae: 1.0155\n",
      "Epoch 40/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 28ms/step - loss: 2.0764 - mae: 1.0902 - val_loss: 1.8229 - val_mae: 1.0169\n",
      "Epoch 41/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 28ms/step - loss: 2.0526 - mae: 1.0831 - val_loss: 1.8283 - val_mae: 1.0227\n",
      "Epoch 42/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 28ms/step - loss: 2.0474 - mae: 1.0822 - val_loss: 1.8313 - val_mae: 1.0197\n",
      "Epoch 43/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 28ms/step - loss: 2.0507 - mae: 1.0838 - val_loss: 1.8474 - val_mae: 1.0316\n",
      "Epoch 44/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 28ms/step - loss: 2.0612 - mae: 1.0863 - val_loss: 1.8181 - val_mae: 1.0175\n",
      "Epoch 45/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 28ms/step - loss: 2.0432 - mae: 1.0824 - val_loss: 1.8255 - val_mae: 1.0150\n",
      "Epoch 46/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 28ms/step - loss: 2.0489 - mae: 1.0852 - val_loss: 1.8133 - val_mae: 1.0075\n",
      "Epoch 47/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 28ms/step - loss: 2.0699 - mae: 1.0844 - val_loss: 1.8312 - val_mae: 1.0197\n",
      "Epoch 48/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 31ms/step - loss: 2.0465 - mae: 1.0805 - val_loss: 1.8068 - val_mae: 1.0098\n",
      "Epoch 49/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 30ms/step - loss: 2.0415 - mae: 1.0805 - val_loss: 1.8200 - val_mae: 1.0188\n",
      "Epoch 50/50\n",
      "\u001b[1m3420/3420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 29ms/step - loss: 2.0343 - mae: 1.0793 - val_loss: 1.8250 - val_mae: 1.0170\n"
     ]
    }
   ],
   "source": [
    "input_shape_cnn = (X_train.shape[1],)\n",
    "cnn_model = create_cnn(input_shape_cnn)\n",
    "cnn_history = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Mean Absolute Error (MAE) no conjunto de teste: 1.0122\n"
     ]
    }
   ],
   "source": [
    "cnn_eval = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"CNN Mean Absolute Error (MAE) no conjunto de teste: {cnn_eval[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN model evaluation shows that it achieves a low MAE of 1.0122 on the test set, demonstrating strong predictive performance. The descriptive statistics of the target variable indicate a mean of approximately 2.0095, with a standard deviation of 2.7165, and a range spanning from -9.9334 to 12.3591. The relative MAE of 4.54% confirms that the prediction error is small compared to the variability in the target variable. These results suggest that the model effectively captures the underlying patterns in the data and provides reliable predictions, making it suitable for biological applications such as modeling drug efficacy or predicting cellular responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE relativo ao intervalo de Y: 4.54%\n"
     ]
    }
   ],
   "source": [
    "mae_relative = cnn_eval[1] / (np.max(target_data) - np.min(target_data))\n",
    "print(f\"MAE relativo ao intervalo de Y: {mae_relative:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo DNN salvo como 'trained_dnn_model.h5'.\n",
      "Modelo CNN salvo como 'trained_cnn_model.h5'.\n"
     ]
    }
   ],
   "source": [
    "# 6. Salvar os modelos treinados\n",
    "dnn_model.save('trained_dnn_model.h5')\n",
    "print(\"Modelo DNN salvo como 'trained_dnn_model.h5'.\")\n",
    "\n",
    "cnn_model.save('trained_cnn_model.h5')\n",
    "print(\"Modelo CNN salvo como 'trained_cnn_model.h5'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4chemoinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
