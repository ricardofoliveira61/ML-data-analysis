{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.multi_pred import DrugRes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator, Descriptors, AllChem, PandasTools\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_GDSC=pd.read_pickle(filepath_or_buffer=\"data/filtered_GDSC.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>HeavyAtomMolWt</th>\n",
       "      <th>ExactMolWt</th>\n",
       "      <th>NumValenceElectrons</th>\n",
       "      <th>FpDensityMorgan3</th>\n",
       "      <th>AvgIpc</th>\n",
       "      <th>...</th>\n",
       "      <th>fr_morpholine</th>\n",
       "      <th>fr_para_hydroxylation</th>\n",
       "      <th>fr_phenol</th>\n",
       "      <th>fr_phenol_noOrthoHbond</th>\n",
       "      <th>fr_piperdine</th>\n",
       "      <th>fr_piperzine</th>\n",
       "      <th>fr_priamide</th>\n",
       "      <th>fr_pyridine</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>150</td>\n",
       "      <td>2.344828</td>\n",
       "      <td>2.745576</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.968757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>150</td>\n",
       "      <td>2.344828</td>\n",
       "      <td>2.745576</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.692768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>150</td>\n",
       "      <td>2.344828</td>\n",
       "      <td>2.745576</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.478678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>150</td>\n",
       "      <td>2.344828</td>\n",
       "      <td>2.745576</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.034050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.857397</td>\n",
       "      <td>5.857397</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>10.517241</td>\n",
       "      <td>393.443</td>\n",
       "      <td>370.259</td>\n",
       "      <td>393.168856</td>\n",
       "      <td>150</td>\n",
       "      <td>2.344828</td>\n",
       "      <td>2.745576</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.966952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167353</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>122</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.210046</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.940836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167354</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>122</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.210046</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.820567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167355</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>122</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.210046</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.785978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167356</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>122</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.210046</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.393454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167357</th>\n",
       "      <td>12.220360</td>\n",
       "      <td>12.220360</td>\n",
       "      <td>-0.169023</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>321.380</td>\n",
       "      <td>302.228</td>\n",
       "      <td>321.147727</td>\n",
       "      <td>122</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.210046</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.099131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167358 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MaxAbsEStateIndex  MaxEStateIndex  MinEStateIndex        SPS    MolWt  \\\n",
       "0                5.857397        5.857397        0.392174  10.517241  393.443   \n",
       "1                5.857397        5.857397        0.392174  10.517241  393.443   \n",
       "2                5.857397        5.857397        0.392174  10.517241  393.443   \n",
       "3                5.857397        5.857397        0.392174  10.517241  393.443   \n",
       "4                5.857397        5.857397        0.392174  10.517241  393.443   \n",
       "...                   ...             ...             ...        ...      ...   \n",
       "167353          12.220360       12.220360       -0.169023  22.500000  321.380   \n",
       "167354          12.220360       12.220360       -0.169023  22.500000  321.380   \n",
       "167355          12.220360       12.220360       -0.169023  22.500000  321.380   \n",
       "167356          12.220360       12.220360       -0.169023  22.500000  321.380   \n",
       "167357          12.220360       12.220360       -0.169023  22.500000  321.380   \n",
       "\n",
       "        HeavyAtomMolWt  ExactMolWt  NumValenceElectrons  FpDensityMorgan3  \\\n",
       "0              370.259  393.168856                  150          2.344828   \n",
       "1              370.259  393.168856                  150          2.344828   \n",
       "2              370.259  393.168856                  150          2.344828   \n",
       "3              370.259  393.168856                  150          2.344828   \n",
       "4              370.259  393.168856                  150          2.344828   \n",
       "...                ...         ...                  ...               ...   \n",
       "167353         302.228  321.147727                  122          2.666667   \n",
       "167354         302.228  321.147727                  122          2.666667   \n",
       "167355         302.228  321.147727                  122          2.666667   \n",
       "167356         302.228  321.147727                  122          2.666667   \n",
       "167357         302.228  321.147727                  122          2.666667   \n",
       "\n",
       "          AvgIpc  ...  fr_morpholine  fr_para_hydroxylation  fr_phenol  \\\n",
       "0       2.745576  ...              0                      0          0   \n",
       "1       2.745576  ...              0                      0          0   \n",
       "2       2.745576  ...              0                      0          0   \n",
       "3       2.745576  ...              0                      0          0   \n",
       "4       2.745576  ...              0                      0          0   \n",
       "...          ...  ...            ...                    ...        ...   \n",
       "167353  3.210046  ...              0                      1          1   \n",
       "167354  3.210046  ...              0                      1          1   \n",
       "167355  3.210046  ...              0                      1          1   \n",
       "167356  3.210046  ...              0                      1          1   \n",
       "167357  3.210046  ...              0                      1          1   \n",
       "\n",
       "        fr_phenol_noOrthoHbond  fr_piperdine  fr_piperzine  fr_priamide  \\\n",
       "0                            0             0             0            0   \n",
       "1                            0             0             0            0   \n",
       "2                            0             0             0            0   \n",
       "3                            0             0             0            0   \n",
       "4                            0             0             0            0   \n",
       "...                        ...           ...           ...          ...   \n",
       "167353                       1             0             1            0   \n",
       "167354                       1             0             1            0   \n",
       "167355                       1             0             1            0   \n",
       "167356                       1             0             1            0   \n",
       "167357                       1             0             1            0   \n",
       "\n",
       "        fr_pyridine  fr_unbrch_alkane         Y  \n",
       "0                 0                 0  3.968757  \n",
       "1                 0                 0  2.692768  \n",
       "2                 0                 0  2.478678  \n",
       "3                 0                 0  2.034050  \n",
       "4                 0                 0  2.966952  \n",
       "...             ...               ...       ...  \n",
       "167353            1                 0  3.940836  \n",
       "167354            1                 0  4.820567  \n",
       "167355            1                 0  5.785978  \n",
       "167356            1                 0  5.393454  \n",
       "167357            1                 0  5.099131  \n",
       "\n",
       "[167358 rows x 142 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_final=filtered_GDSC.select_dtypes(include=[np.number])\n",
    "dataset_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enconding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to prepare the dataset for deep learning by encoding specific features, such as the Tissue, Morgan Fingerprints and Gene expression, to ensure compatibility with algorithms that require numerical input. Initially, the Tissue, gene Expression and Morgan Fingerprints columns are extracted from the dataset to isolate the data that requires transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_data = filtered_GDSC['Tissue'].iloc[:25000]\n",
    "gene = filtered_GDSC['Gene_expression'].iloc[:25000]\n",
    "morgan_fingerprints = filtered_GDSC['morgan_fingerprints'].iloc[:25000]\n",
    "dataset_filtrado = dataset_final.iloc[:25000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Morgan Fingerprints and gene expression are then expanded into a structured numerical format. Each, typically stored as a list or array, is unpacked into individual columns (morgan_0, morgan_1, etc.) to ensure compatibility with deep learning models, which require fixed-size numerical input. While this approach captures detailed molecular information, it also increases the dimensionality of the dataset, which may result in higher computational demands. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_encodings = np.vstack(gene)\n",
    "gene_columns = [f'gene_{i}' for i in range(gene_encodings.shape[1])]\n",
    "gene_df = pd.DataFrame(gene_encodings, columns=gene_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "morgan_encodings = np.vstack(morgan_fingerprints)\n",
    "morgan_columns = [f'morgan_{i}' for i in range(morgan_encodings.shape[1])]\n",
    "morgan_df = pd.DataFrame(morgan_encodings, columns=morgan_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tissue column undergoes one-hot encoding, transforming the categorical data into binary format. Each unique tissue type is represented as a separate column, with each row containing a 1 for the corresponding tissue type and 0s elsewhere. This step ensures that categorical information is preserved and converted into a numerical format without introducing any ordinal assumptions. While this process increases the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "tissue_encodings = one_hot_encoder.fit_transform(tissue_data.values.reshape(-1, 1))\n",
    "tissue_columns = [f'tissue_{cat}' for cat in one_hot_encoder.categories_[0]]\n",
    "tissue_df = pd.DataFrame(tissue_encodings, columns=tissue_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing the Tissue, Gene Expression and Morgan Fingerprints data, they are concatenated with the filtered dataset, creating an augmented dataset that integrates molecular and biological features. This comprehensive dataset is ready for model training and ensures that all relevant data is combined into one unified structure. However, the integration increases the overall size of the dataset, which can require more computational resources. The combined dataset allows the model to leverage both the chemical and biological contexts, enhancing its predictive capabilities and making it more robust for complex tasks such as predicting drug efficacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = pd.concat([dataset_filtrado, morgan_df, gene_df, tissue_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, its necessary splitting the dataset into training and testing sets and normalizing the features to ensure compatibility with the algorithms. First, the target variable (Y), which represents the outcome or label to predict, is separated from the feature set. The target variable is extracted as target_data, while the rest of the dataset is stored as augmented_data. This separation ensures that the deep learning model can be trained with input features and validated against the target labels without introducing leakage. In the context of bioinformatics, the target often represents biological responses, such as drug efficacy, making its isolation critical for meaningful predictions. The dataset is then split into training and testing sets using the train_test_split function. The training set, comprising 80% of the data, is used to train the model, while the remaining 20% is reserved for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas no dataset augmentado (X): 25000, Colunas: 15551\n",
      "Linhas no target_data (Y): 25000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target_data = augmented_data['Y'].values  \n",
    "augmented_data = augmented_data.drop(columns=['Y'])  \n",
    "\n",
    "\n",
    "print(f\"Linhas no dataset augmentado (X): {augmented_data.shape[0]}, Colunas: {augmented_data.shape[1]}\")\n",
    "print(f\"Linhas no target_data (Y): {target_data.shape[0]}\")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_data.values, target_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the features are normalized using the StandardScaler, which standardizes the data by scaling it to have a zero mean and unit variance. The scaler is fitted on the training set and then applied to both the training and testing sets. This ensures that the testing data is scaled consistently with the training data, preventing data leakage. Normalization is essential because many machine learning algorithms are sensitive to the scale of input features, and standardizing them improves model convergence and performance. In biological datasets, where features may include diverse molecular and tissue-related data with varying ranges, normalization ensures that no single feature dominates due to its magnitude, allowing the model to focus on meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "its time to defines, compiles, and trains a Deep Neural Network (DNN) for predicting outcomes based on the input dataset. The first part defines the DNN architecture using a function called create_dnn. The model is built sequentially and consists of three layers: the first layer is a dense (fully connected) layer with 128 units and ReLU activation, followed by a dropout layer that randomly drops 30% of the connections during training to reduce overfitting. The second layer is another dense layer with 64 units and ReLU activation, also followed by a dropout layer. The final layer is a single-unit dense layer with linear activation, which is suitable for regression tasks where the goal is to predict continuous values. The model is compiled using the Adam optimizer, which adjusts learning rates dynamically to improve convergence, mean squared error (MSE) as the loss function to minimize, and mean absolute error (MAE) as a performance metric for evaluation. This architecture is particularly designed for regression tasks that involve complex, non-linear relationships, making it well-suited for biological applications, such as predicting drug responses or molecular activities. The dropout layers add robustness by preventing the model from overfitting the training data, while the use of ReLU activation ensures computational efficiency and avoids the vanishing gradient problem common in deeper networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "def create_dnn(input_shape):\n",
    "    model = Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    print(\"DNN Summary:\")\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_dnn function is then used to instantiate the model. The model is trained using the fit method, with 20% of the training data reserved for validation to monitor the model's generalization performance. The training is conducted over 50 epochs (iterations over the dataset) with a batch size of 32, meaning the model updates its weights after every 32 samples. The verbose output allows real-time monitoring of the training and validation performance metrics. This training process adjusts the model's parameters (weights and biases) to minimize the error on the training data while ensuring it generalizes well to unseen data through validation monitoring. The combination of layers, dropout, and the Adam optimizer creates a robust and efficient model capable of learning complex patterns in the dataset. However, the model's performance heavily depends on the choice of hyperparameters, such as the number of layers, dropout rates, and learning rates, which need to be carefully tuned. In the context of biological datasets, this DNN architecture can effectively predict outcomes such as drug sensitivity or therapeutic responses by leveraging molecular and tissue-specific features, capturing the underlying biological variability in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,990,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,990,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,998,977</span> (7.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,998,977\u001b[0m (7.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,998,977</span> (7.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,998,977\u001b[0m (7.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 13.9628 - mae: 2.8162 - val_loss: 2.6520 - val_mae: 1.2523\n",
      "Epoch 2/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 3.2110 - mae: 1.4080 - val_loss: 1.8528 - val_mae: 1.0495\n",
      "Epoch 3/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 2.3734 - mae: 1.1935 - val_loss: 1.8020 - val_mae: 1.0388\n",
      "Epoch 4/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.9662 - mae: 1.0927 - val_loss: 1.6464 - val_mae: 0.9886\n",
      "Epoch 5/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.8917 - mae: 1.0594 - val_loss: 1.4976 - val_mae: 0.9406\n",
      "Epoch 6/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 1.8365 - mae: 1.0459 - val_loss: 1.6450 - val_mae: 0.9653\n",
      "Epoch 7/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.7282 - mae: 1.0119 - val_loss: 1.4746 - val_mae: 0.9212\n",
      "Epoch 8/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 1.6804 - mae: 1.0013 - val_loss: 1.6415 - val_mae: 1.0095\n",
      "Epoch 9/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 1.6134 - mae: 0.9789 - val_loss: 1.4505 - val_mae: 0.9194\n",
      "Epoch 10/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.5961 - mae: 0.9675 - val_loss: 1.4360 - val_mae: 0.9144\n",
      "Epoch 11/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 1.5812 - mae: 0.9647 - val_loss: 1.4610 - val_mae: 0.9183\n",
      "Epoch 12/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.5768 - mae: 0.9587 - val_loss: 1.2961 - val_mae: 0.8622\n",
      "Epoch 13/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 1.5459 - mae: 0.9589 - val_loss: 1.5841 - val_mae: 0.9858\n",
      "Epoch 14/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.5635 - mae: 0.9658 - val_loss: 1.3748 - val_mae: 0.8927\n",
      "Epoch 15/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 1.4626 - mae: 0.9334 - val_loss: 1.3955 - val_mae: 0.9078\n",
      "Epoch 16/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.4748 - mae: 0.9349 - val_loss: 1.3476 - val_mae: 0.8760\n",
      "Epoch 17/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.4505 - mae: 0.9363 - val_loss: 1.3862 - val_mae: 0.8903\n",
      "Epoch 18/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.4067 - mae: 0.9173 - val_loss: 1.5499 - val_mae: 0.9579\n",
      "Epoch 19/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.4103 - mae: 0.9225 - val_loss: 1.3798 - val_mae: 0.9126\n",
      "Epoch 20/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.3735 - mae: 0.9041 - val_loss: 1.3014 - val_mae: 0.8718\n",
      "Epoch 21/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.3648 - mae: 0.9042 - val_loss: 1.3534 - val_mae: 0.8846\n",
      "Epoch 22/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.2838 - mae: 0.8753 - val_loss: 1.3036 - val_mae: 0.8722\n",
      "Epoch 23/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.3189 - mae: 0.8868 - val_loss: 1.2813 - val_mae: 0.8625\n",
      "Epoch 24/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.3013 - mae: 0.8807 - val_loss: 1.2432 - val_mae: 0.8458\n",
      "Epoch 25/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.2980 - mae: 0.8719 - val_loss: 1.5928 - val_mae: 0.9528\n",
      "Epoch 26/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.2802 - mae: 0.8692 - val_loss: 1.2514 - val_mae: 0.8611\n",
      "Epoch 27/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 1.2168 - mae: 0.8434 - val_loss: 1.3379 - val_mae: 0.8773\n",
      "Epoch 28/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.2421 - mae: 0.8518 - val_loss: 1.2784 - val_mae: 0.8772\n",
      "Epoch 29/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.2002 - mae: 0.8469 - val_loss: 1.3900 - val_mae: 0.9102\n",
      "Epoch 30/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.2175 - mae: 0.8533 - val_loss: 1.2772 - val_mae: 0.8535\n",
      "Epoch 31/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.2072 - mae: 0.8467 - val_loss: 1.2559 - val_mae: 0.8505\n",
      "Epoch 32/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1584 - mae: 0.8286 - val_loss: 1.3145 - val_mae: 0.8829\n",
      "Epoch 33/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1540 - mae: 0.8275 - val_loss: 1.1987 - val_mae: 0.8210\n",
      "Epoch 34/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1599 - mae: 0.8264 - val_loss: 1.2913 - val_mae: 0.8728\n",
      "Epoch 35/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1672 - mae: 0.8248 - val_loss: 1.1977 - val_mae: 0.8246\n",
      "Epoch 36/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1366 - mae: 0.8247 - val_loss: 1.2605 - val_mae: 0.8453\n",
      "Epoch 37/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 1.0822 - mae: 0.7995 - val_loss: 1.2088 - val_mae: 0.8272\n",
      "Epoch 38/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1143 - mae: 0.8126 - val_loss: 1.1756 - val_mae: 0.8261\n",
      "Epoch 39/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.0973 - mae: 0.8074 - val_loss: 1.2386 - val_mae: 0.8523\n",
      "Epoch 40/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1220 - mae: 0.8181 - val_loss: 1.2719 - val_mae: 0.8550\n",
      "Epoch 41/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1032 - mae: 0.8116 - val_loss: 1.3479 - val_mae: 0.8957\n",
      "Epoch 42/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.0660 - mae: 0.7859 - val_loss: 1.1726 - val_mae: 0.8123\n",
      "Epoch 43/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.0641 - mae: 0.7930 - val_loss: 1.3113 - val_mae: 0.8859\n",
      "Epoch 44/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.0415 - mae: 0.7822 - val_loss: 1.2130 - val_mae: 0.8416\n",
      "Epoch 45/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.0309 - mae: 0.7787 - val_loss: 1.4403 - val_mae: 0.9086\n",
      "Epoch 46/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.0647 - mae: 0.7889 - val_loss: 1.3168 - val_mae: 0.8944\n",
      "Epoch 47/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.0755 - mae: 0.7945 - val_loss: 1.2368 - val_mae: 0.8383\n",
      "Epoch 48/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.0481 - mae: 0.7862 - val_loss: 1.1846 - val_mae: 0.8110\n",
      "Epoch 49/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.9980 - mae: 0.7671 - val_loss: 1.1495 - val_mae: 0.8089\n",
      "Epoch 50/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.9969 - mae: 0.7670 - val_loss: 1.2776 - val_mae: 0.8752\n"
     ]
    }
   ],
   "source": [
    "input_shape_dnn = (X_train.shape[1],)\n",
    "\n",
    "dnn_model = create_dnn(input_shape_dnn)\n",
    "\n",
    "dnn_history = dnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code evaluates the performance of the trained Deep Neural Network (DNN) model on the test dataset and contextualizes the results by analyzing the target variable's distribution. First, the model's performance is assessed using the evaluate function, which calculates the Mean Absolute Error (MAE) on the test set. The MAE quantifies the average absolute difference between the predicted and actual values, providing a straightforward measure of the model's accuracy. In this case, the reported MAE on the test set is 0.8769, indicating the average prediction error. This step is essential to assess how well the model generalizes to unseen data and provides a practical sense of prediction accuracy in the same units as the target variable. The MAE is particularly useful for regression tasks as it is not overly sensitive to outliers, unlike other metrics like Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Mean Absolute Error (MAE) no conjunto de teste: 0.8769\n"
     ]
    }
   ],
   "source": [
    "dnn_eval = dnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"DNN Mean Absolute Error (MAE) no conjunto de teste: {dnn_eval[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This percentage indicates that the average error is only 5.20% of the total range of the target variable. This suggests that the model's performance is strong, with relatively small errors in the context of the overall data scale. Overall, the results indicate a well-performing model with low prediction errors relative to the variability in the target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE relativo ao intervalo de Y: 5.20%\n"
     ]
    }
   ],
   "source": [
    "mae_relative = dnn_eval[1] / (np.max(target_data) - np.min(target_data))\n",
    "print(f\"MAE relativo ao intervalo de Y: {mae_relative:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(target_data, bins=50, edgecolor='k')\n",
    "plt.title(\"Distribuição da Variável Alvo (Y)\")\n",
    "plt.xlabel(\"Valores de Y\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram illustrates the distribution of the target variable \\( Y \\) in the dataset. The x-axis represents the range of \\( Y \\) values, spanning approximately from -6.8 to 6.8, while the y-axis indicates the frequency of occurrences for each interval of \\( Y \\). The distribution appears to be unimodal and slightly skewed to the left, with the majority of \\( Y \\) values concentrated between 0 and 3. The peak frequency occurs near 2, as evidenced by the tallest bars in the histogram. At the extremes of the range, near -6 and 6, the frequency of \\( Y \\) values is notably lower, indicating that such values are rare in the dataset. This visualization aligns with the previously calculated descriptive statistics, providing a clear and concise overview of the distribution of \\( Y \\) values and their variability across the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_cnn function defines the CNN architecture. It begins by reshaping the input data to a format suitable for convolutional operations, treating the features as a 1D sequence. The model includes two 1D convolutional layers with 32 and 64 filters, respectively, each using a kernel size of 3 and ReLU activation. These layers extract local patterns from the input data, such as dependencies or correlations between features. Each convolutional layer is followed by a MaxPooling layer, which downsamples the data to reduce its dimensionality while retaining the most significant features, thereby improving computational efficiency.\n",
    "\n",
    "After the convolutional layers, the data is flattened into a single vector to prepare it for the fully connected layers. A dense layer with 64 neurons and ReLU activation is included to learn high-level feature representations. A dropout layer with a 30% dropout rate follows, helping to reduce overfitting by randomly deactivating neurons during training. The final layer is a dense output layer with one neuron and a linear activation function, which is appropriate for regression tasks where the output is a continuous variable. The model is compiled using the Adam optimizer, which dynamically adjusts learning rates for faster convergence, the mean squared error (MSE) as the loss function, and the mean absolute error (MAE) as a performance metric for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(input_shape):\n",
    "    model = Sequential([\n",
    "        layers.Reshape((input_shape[0], 1), input_shape=input_shape),\n",
    "        layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    print(\"CNN Summary:\")\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_cnn function is called to instantiate the model, and the model is trained using the fit method. During training, 20% of the data is reserved for validation, allowing the model's generalization performance to be monitored. The training runs for 50 epochs, with a batch size of 32, enabling frequent weight updates and faster convergence. The training process adjusts the model's parameters, including filters, weights, and biases, to minimize the error on the training data while maintaining good performance on the validation set.\n",
    "\n",
    "This CNN architecture is particularly advantageous because it leverages convolutional layers to extract hierarchical patterns, making it ideal for datasets with complex dependencies. The pooling layers and dropout further enhance its generalization by reducing overfitting and computational complexity. However, training CNNs can be computationally intensive, and the choice of hyperparameters, such as the number of filters, kernel size, and dropout rate, significantly impacts performance. From a biological perspective, this CNN model is well-suited for analyzing molecular descriptors or gene expression data, as it can capture local dependencies and intricate relationships between features. This makes it highly effective for tasks like predicting drug efficacy or modeling biological responses, where understanding subtle patterns in the data is crucial for generating meaningful predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15551</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15549</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7774</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7772</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3886</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">248704</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │    <span style=\"color: #00af00; text-decoration-color: #00af00\">15,917,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15551\u001b[0m, \u001b[38;5;34m1\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15549\u001b[0m, \u001b[38;5;34m32\u001b[0m)      │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7774\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7772\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │         \u001b[38;5;34m6,208\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3886\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m248704\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │    \u001b[38;5;34m15,917,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,923,521</span> (60.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,923,521\u001b[0m (60.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,923,521</span> (60.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,923,521\u001b[0m (60.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 214ms/step - loss: 52.3787 - mae: 3.4644 - val_loss: 4.1782 - val_mae: 1.5623\n",
      "Epoch 2/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 214ms/step - loss: 4.9376 - mae: 1.6645 - val_loss: 4.1371 - val_mae: 1.5558\n",
      "Epoch 3/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 213ms/step - loss: 5.1191 - mae: 1.6994 - val_loss: 3.9030 - val_mae: 1.4781\n",
      "Epoch 4/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 207ms/step - loss: 4.8665 - mae: 1.6501 - val_loss: 4.2506 - val_mae: 1.6049\n",
      "Epoch 5/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 209ms/step - loss: 4.9413 - mae: 1.6570 - val_loss: 3.9086 - val_mae: 1.4909\n",
      "Epoch 6/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 208ms/step - loss: 4.9803 - mae: 1.6660 - val_loss: 3.7570 - val_mae: 1.4470\n",
      "Epoch 7/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 209ms/step - loss: 4.3860 - mae: 1.5556 - val_loss: 2.0677 - val_mae: 1.1188\n",
      "Epoch 8/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 208ms/step - loss: 2.8889 - mae: 1.2954 - val_loss: 1.5904 - val_mae: 0.9612\n",
      "Epoch 9/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 211ms/step - loss: 2.5522 - mae: 1.2250 - val_loss: 1.5616 - val_mae: 0.9488\n",
      "Epoch 10/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 212ms/step - loss: 2.2762 - mae: 1.1608 - val_loss: 1.5583 - val_mae: 0.9570\n",
      "Epoch 11/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 210ms/step - loss: 2.1901 - mae: 1.1391 - val_loss: 1.7198 - val_mae: 1.0137\n",
      "Epoch 12/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 209ms/step - loss: 2.1287 - mae: 1.1147 - val_loss: 1.4538 - val_mae: 0.9180\n",
      "Epoch 13/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 209ms/step - loss: 1.9581 - mae: 1.0705 - val_loss: 1.5165 - val_mae: 0.9431\n",
      "Epoch 14/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 212ms/step - loss: 1.8859 - mae: 1.0471 - val_loss: 1.4326 - val_mae: 0.9073\n",
      "Epoch 15/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 217ms/step - loss: 1.7571 - mae: 1.0207 - val_loss: 1.5824 - val_mae: 0.9680\n",
      "Epoch 16/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 226ms/step - loss: 1.6756 - mae: 0.9937 - val_loss: 1.4254 - val_mae: 0.9062\n",
      "Epoch 17/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 215ms/step - loss: 1.6822 - mae: 0.9869 - val_loss: 1.6800 - val_mae: 0.9815\n",
      "Epoch 18/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 212ms/step - loss: 1.6814 - mae: 0.9870 - val_loss: 1.3286 - val_mae: 0.8640\n",
      "Epoch 19/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 211ms/step - loss: 1.5961 - mae: 0.9648 - val_loss: 1.4061 - val_mae: 0.8940\n",
      "Epoch 20/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 213ms/step - loss: 1.5701 - mae: 0.9605 - val_loss: 1.3308 - val_mae: 0.8727\n",
      "Epoch 21/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 212ms/step - loss: 1.5675 - mae: 0.9505 - val_loss: 1.2868 - val_mae: 0.8536\n",
      "Epoch 22/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 212ms/step - loss: 1.5417 - mae: 0.9432 - val_loss: 1.3191 - val_mae: 0.8686\n",
      "Epoch 23/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 213ms/step - loss: 1.4895 - mae: 0.9239 - val_loss: 1.2915 - val_mae: 0.8608\n",
      "Epoch 24/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 211ms/step - loss: 1.4889 - mae: 0.9229 - val_loss: 1.2595 - val_mae: 0.8482\n",
      "Epoch 25/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 211ms/step - loss: 1.4074 - mae: 0.9028 - val_loss: 1.3121 - val_mae: 0.8728\n",
      "Epoch 26/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 210ms/step - loss: 1.3868 - mae: 0.8986 - val_loss: 1.3597 - val_mae: 0.8965\n",
      "Epoch 27/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 211ms/step - loss: 1.3974 - mae: 0.8904 - val_loss: 1.2617 - val_mae: 0.8457\n",
      "Epoch 28/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 210ms/step - loss: 1.3305 - mae: 0.8799 - val_loss: 1.2823 - val_mae: 0.8555\n",
      "Epoch 29/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 213ms/step - loss: 1.3366 - mae: 0.8802 - val_loss: 1.2680 - val_mae: 0.8477\n",
      "Epoch 30/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 215ms/step - loss: 1.3480 - mae: 0.8776 - val_loss: 1.2983 - val_mae: 0.8650\n",
      "Epoch 31/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 212ms/step - loss: 1.2999 - mae: 0.8686 - val_loss: 1.2342 - val_mae: 0.8329\n",
      "Epoch 32/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 220ms/step - loss: 1.3199 - mae: 0.8638 - val_loss: 1.2025 - val_mae: 0.8212\n",
      "Epoch 33/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 225ms/step - loss: 1.2982 - mae: 0.8612 - val_loss: 1.2602 - val_mae: 0.8514\n",
      "Epoch 34/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 221ms/step - loss: 1.2935 - mae: 0.8556 - val_loss: 1.2272 - val_mae: 0.8380\n",
      "Epoch 35/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 213ms/step - loss: 1.2602 - mae: 0.8440 - val_loss: 1.2392 - val_mae: 0.8425\n",
      "Epoch 36/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 212ms/step - loss: 1.2084 - mae: 0.8330 - val_loss: 1.2025 - val_mae: 0.8226\n",
      "Epoch 37/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 213ms/step - loss: 1.2416 - mae: 0.8321 - val_loss: 1.2653 - val_mae: 0.8500\n",
      "Epoch 38/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 211ms/step - loss: 1.2164 - mae: 0.8236 - val_loss: 1.2025 - val_mae: 0.8207\n",
      "Epoch 39/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 211ms/step - loss: 1.1872 - mae: 0.8195 - val_loss: 1.2002 - val_mae: 0.8242\n",
      "Epoch 40/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 212ms/step - loss: 1.2002 - mae: 0.8157 - val_loss: 1.2196 - val_mae: 0.8332\n",
      "Epoch 41/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 212ms/step - loss: 1.1365 - mae: 0.7999 - val_loss: 1.2435 - val_mae: 0.8402\n",
      "Epoch 42/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 213ms/step - loss: 1.1147 - mae: 0.7897 - val_loss: 1.1820 - val_mae: 0.8157\n",
      "Epoch 43/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 214ms/step - loss: 1.1514 - mae: 0.8104 - val_loss: 1.2179 - val_mae: 0.8330\n",
      "Epoch 44/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 214ms/step - loss: 1.1448 - mae: 0.7963 - val_loss: 1.1988 - val_mae: 0.8249\n",
      "Epoch 45/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 215ms/step - loss: 1.1067 - mae: 0.7877 - val_loss: 1.1882 - val_mae: 0.8168\n",
      "Epoch 46/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 217ms/step - loss: 1.1233 - mae: 0.7913 - val_loss: 1.1856 - val_mae: 0.8122\n",
      "Epoch 47/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 217ms/step - loss: 1.1126 - mae: 0.7850 - val_loss: 1.2196 - val_mae: 0.8301\n",
      "Epoch 48/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 220ms/step - loss: 1.0507 - mae: 0.7710 - val_loss: 1.1878 - val_mae: 0.8151\n",
      "Epoch 49/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 230ms/step - loss: 1.1033 - mae: 0.7875 - val_loss: 1.2059 - val_mae: 0.8257\n",
      "Epoch 50/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 219ms/step - loss: 1.0779 - mae: 0.7774 - val_loss: 1.2037 - val_mae: 0.8251\n"
     ]
    }
   ],
   "source": [
    "input_shape_cnn = (X_train.shape[1],)\n",
    "cnn_model = create_cnn(input_shape_cnn)\n",
    "cnn_history = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Mean Absolute Error (MAE) no conjunto de teste: 0.8195\n"
     ]
    }
   ],
   "source": [
    "cnn_eval = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"CNN Mean Absolute Error (MAE) no conjunto de teste: {cnn_eval[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN Mean Absolute Error (MAE) on the test set, reported as 0.8195, represents the average absolute difference between the model's predictions and the actual values. It indicates that, on average, the predictions are 0.8195 units away from the true values in the test data. A lower MAE reflects better model performance, and this value suggests a moderate level of accuracy depending on the range of the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE relativo ao intervalo de Y: 4.86%\n"
     ]
    }
   ],
   "source": [
    "mae_relative = cnn_eval[1] / (np.max(target_data) - np.min(target_data))\n",
    "print(f\"MAE relativo ao intervalo de Y: {mae_relative:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This percentage indicates that the average error is only 4.86% of the total range of the target variable. This suggests that the model's performance is strong, with relatively small errors in the context of the overall data scale. Overall, the results indicate a well-performing model with low prediction errors relative to the variability in the target data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
